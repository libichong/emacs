#+OPTIONS: toc:nil ^:nil author:nil date:nil html-postamble:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />
#+TITLE: System Design Notes

** [[https://media.netflix.com/en/company-blog/how-netflix-works-with-isps-around-the-globe-to-deliver-a-great-viewing-experience][How Netflix Works With ISPs Around the Globe to Deliver a Great Viewing Experience]]
- Netflix Open Connect
- Open Connect Appliances
- copy each file once from our US-based transcoding repository to the storage locations within Australia
- [[https://www-users.cs.umn.edu/~viadhi/netflix.pdf][Unreeling Netflix: Understanding and Improving Multi-CDN Movie Delivery]]
** [[http://www.puncsky.com/blog/2016/02/14/crack-the-system-design-interview/][Crack the System Design Interview]]
Breaking down a complex task into small chunks helps us handle the problem at a better pace and in a more actionable way.
*** Clarify Requirements and Specs
   the ultimate goals should always be clear. Pinterest is a highly scalable photo-sharing service:
   - features: user profile, upload photos, news feed, etc.
   - scaling out: horizontal scalability and micro services.
*** Sketch Out High Level Design
   =Do not dive into details before outlining the big picture.= Otherwise, going off too far towards a wrong direction would make it harder to even provide a roughly correct solution. We will regret wasting time on irrelevant details when we do not have time to finish the task.
*** Discuss individual components and how they interact in detail
identify what each component is and explain how they interact with one another.
**** Load Balancer
suggest exact algorithms like round robin, weighted round robin, least loaded, least loaded with slow start, utilization limit, latency, cascade, etc.
- DNS Round Robin (rarely used): clients get a randomly-ordered list of IP addresses.
    pros: easy to implement and free
    cons: hard to control and not responsive, since DNS cache needs time to expire
- L3/L4 Load Balancer: traffic is routed by IP address and port. L3 is network layer (IP). L4 is session layer (TCP).
    pros: better granularity, simple, responsive
- L7 Load Balancer: traffic is routed by what is inside the HTTP protocol. L7 is application layer (HTTP).
**** Reverse Proxy
Reverse proxy, like varnish, centralizes internal services and provides unified interfaces to the public. For example, www.example.com/index and www.example.com/sports appear to come from the same domain, but in fact they are from different micro services behind the reverse proxy. Reverse proxy could also help with caching and load balancing.
**** (Frontend) Web Tier
web pages are served, and usually combined with the service / backend tier in the very early stage of a web service.

- Stateless

    There are two major bottlenecks of the whole system – requests per second (rps) and bandwidth. We could improve the situation by using more efficient tech stack, like frameworks with async and non-blocking reactor pattern, and enhancing the hardware, like scaling up (aka vertical scaling) or scaling out (aka horizontal scaling).

    Internet companies prefer scaling out, since it is more cost-efficient with a huge number of commodity machines. This is also good for recruiting, because the target skillsets are equipped by. After all, people rarely play with super computers or mainframes at home.

    Frontend web tier and service tier must be stateless in order to add or remove hosts conveniently, thus achieving horizontal scalability. As for feature switch or configs, there could be a database table / standalone service to keep those states.

- Web Application and API

    MVC(MVT) or MVVC pattern is the dominant pattern for this layer. Traditionally, view or template is rendered to HTML by the server at runtime. In the age of mobile computing, view can be as simple as serving the minimal package of data transporting to the mobile devices, which is called web API. People believe that the API can be shared by clients and browsers. And that is why single page web applications are becoming more and more popular, especially with the assistance of frontend frameworks like react.js, angular.js, backbone.js, etc.

**** App Service Tier

  The single responsibility principle advocates small and autonomous services that work together, so that each service can do one thing well and not block others. Small teams with small services can plan much more aggressively for the sake of hyper-growth.
1) Service Discovery

   Zookeeper is a popular and centralized choice. Instances with name, address, port, etc. are registered into the path in ZooKeeper for each service. If one service does not know where to find another service, it can query Zookeeper for the location and memorize it until that location is unavailable.

   Zookeeper is a CP system in terms of CAP theorem (See Section 2.3 for more discussion), which means it stays consistent in the case of failures, but the leader of the centralized consensus will be unavailable for registering new services.

   In contrast to Zookeeper, Uber is doing interesting work in a decentralized way, named hyperbahn, based on Ringpop consisten hash ring. Read Amazon’s Dynamo to understand AP and eventual consistency.

2) Micro Services

   For the Pinterest case, these micro services could be user profile, follower, feed, search, spam, etc. Any of those topics could lead to an in-depth discussion. Useful links are listed in Section 3: Future Studies, to help us deal with them.

   However, for a general interview question like “design Pinterest”, it is good enough to leave those services as black boxes.. If we want to show more passion, elaborate with some sample endpoints / APIs for those services would be great.

**** Data Tier

Although a relational database can do almost all the storage work, please remember do not save a blob, like a photo, into a relational database, and choose the right database for the right service. For example, read performance is important for follower service, therefore it makes sense to use a key-value cache. Feeds are generated as time passes by, so HBase / Cassandra’s timestamp index is a great fit for this use case. Users have relationships with other users or objects, so a relational database is our choice by default in an user profile service.

Data and storage is a rather wide topic, and we will discuss it later in Section 2.2 Storage.
*** Back-of-the-envelope Calculation

The final step, estimating how many machines are required, is optional, because time is probably up after all the discussions above and three common topics below. In case we run into this topic, we’d better prepare for it as well. It is a little tricky… we need come up with some variables and a function first, and then make some guesses for the values of those variables, and finally calculate the result.

The cost is a function of CPU, RAM, storage, bandwidth, number and size of the images uploaded each day.

- /N/ users 10^10
- /i/ images / (user * day) 10
- /s/ size in bytes / image 10^6
- viewed /v/ times / image 100
- d days
- /h/ requests / sec 10^4 (bottleneck)
- b bandwidth (bottleneck)
- Server cost: $1000 / server
- Storage cost: $0.1 / GB
- Storage = Nisd
Remember the two bottlenecks we mentioned in section 1.3.3 Web Tier? – requests per second (rps) and bandwidth. So the final expression would be

Number of required servers = max(Niv/h, Nivs/b)
*** Three Common Topics
*** Commnication

Stub procedure: a local procedure that marshals the procedure identifier and the arguments into a request message, and then to send via its communication module to the server. When the reply message arrives, it unmarshals the results.

RPC protocols

- Google Protobuf: an open source RPC with only APIs but no RPC implementations. Smaller serialized data and slightly faster. Better documentations and cleaner APIs.
- Facebook Thrift: supports more languages, richer data structures: list, set, map, etc. that Protobuf does not support) Incomplete documentation and hard to find good examples. User case: Hbase/Cassandra/Hypertable/Scrib/..
- Apache Avro: Avro is heavily used in the hadoop ecosystem and based on dynamic schemas in Json. It features dynamic typing, untagged data, and no manually-assigned field IDs.

Generally speaking, RPC is internally used by many tech companies for performance issues, but it is rather hard to debug and not flexible. So for public APIs, we tend to use HTTP APIs, and are usually following the RESTful style.

REST (Representational state transfer of resources)
- Best practice of HTTP API to interact with resources.
- URL only decides the location. Headers (Accept and Content-Type, etc.) decide the representation. HTTP methods(GET/POST/PUT/DELETE) decide the state transfer.
- minimize the coupling between client and server (a huge number of HTTP infras on various clients, data-marshalling).
- stateless and scaling out.
- service partitioning feasible.
- used for public API.

*** Storage
**** Relational Database
ACID (atomicity, consistency, isolation, and durability)
***** Schema Design and 3rd Normal Form (3NF)
To reduce redundancy and improve consistency, people follow 3NF when designing database schemas:

+ 1NF: tabular, each row-column intersection contains only one value
+ 2NF: only the primary key determines all the attributes
+ 3NF: only the candidate keys determine all the attributes (and non-prime attributes do not depend on each other)
**** Db Proxy
What if we want to eliminate single point of failure? What if the dataset is too large for one single machine to hold? For MySQL, the answer is to use a DB proxy to distribute data, either by [[http://dba.stackexchange.com/questions/8889/mysql-sharding-vs-mysql-cluster][clustering or by sharding]].

Clustering is a decentralized solution. Everything is automatic. Data is distributed, moved, rebalanced automatically. Nodes gossip with each other, (though it may cause group isolation).

Sharding is a centralized solution. If we get rid of properties of clustering that we don’t like, sharding is what we get. Data is distributed manually and does not move. Nodes are not aware of each other.

**** NoSQL
***** Key-value Store
The abstraction of a KV store is a giant hashtable/hashmap/dictionary.

The main reason we want to use a key-value cache is to reduce latency for accessing active data. Achieve an O(1) read/write performance on a fast and expensive media (like memory or SSD), instead of a traditional O(logn) read/write on a slow and cheap media (typically hard drive).

There are three major factors to consider when we design the cache.

Pattern: How to cache? is it read-through/write-through/write-around/write-back/cache-aside?
Placement: Where to place the cache? client side/distinct layer/server side?
Replacement: When to expire/replace the data? LRU/LFU/ARC?
Out-of-box choices: Redis/Memcache? Redis supports data persistence while memcache does not. Riak, Berkeley DB, HamsterDB, Amazon Dynamo, Project Voldemort, etc.

***** Document Store
documents, like XML, JSON, BSON, and so on
Out-of-box choices: MongoDB, CouchDB, Terrastore, OrientDB, RavenDB, etc.

***** Column-oriented Store
like a giant nested map: ColumnFamily<RowKey, Columns<Name, Value, Timestamp>>.

The main reason we want to use a column-oriented store is that it is distributed, highly-available, and optimized for write.

Out-of-box choices: Cassandra, HBase, Hypertable, Amazon SimpleDB, etc.

***** Graph Database

As the name indicates, this database’s abstraction is a graph. It allows us to store entities and the relationships between them.

If we use a relational database to store the graph, adding/removing relationships may involve schema changes and data movement, which is not the case when using a graph database. On the other hand, when we create tables in a relational database for the graph, we model based on the traversal we want; if the traversal changes, the data will have to change.

Out-of-box choices: Neo4J, Infinitegraph, OrientDB, FlockDB, etc.

*** CAP Theorem
trading off among CAP (consistency, availability, and partition tolerance) is almost the first thing we want to consider.

+ Consistency: all nodes see the same data at the same time
+ Availability: a guarantee that every request receives a response about whether it succeeded or failed
+ Partition tolerance: system continues to operate despite arbitrary message loss or failure of part of the system.

In a distributed context, the choice is between CP and AP. Unfortunately, CA is just a joke, because single point of failure is a red flag in the real distributed systems world.

To ensure consistency, there are some popular protocols to consider: 2PC, eventual consistency (vector clock + RWN), Paxos, [[http://www.confluent.io/blog/hands-free-kafka-replication-a-lesson-in-operational-simplicity/][In-Sync Replica]], etc.

To ensure availability, we can add replicas for the data. As to components of the whole system, people usually do [[https://www.ibm.com/developerworks/community/blogs/RohitShetty/entry/high_availability_cold_warm_hot?lang=en][cold standby, warm standby, hot standby, and active-active]] to handle the failover.

*** Future Studies
+ [[https://docs.google.com/document/d/1dNKjHICogW5f94MKoBr8wDA3TASbhP0nrcy4eKiuA8Q/edit?usp=sharing][Tian’s notes on big data from a programmer’s perspective]]
+ [[https://www.linkedin.com/pulse/100-open-source-big-data-architecture-papers-anil-madan][100 open source big data architecture papers]]
+ [[https://github.com/checkcheckzz/system-design-interview][System design interview for IT companies]]
+ [[http://www.amazon.com/The-Practice-Cloud-System-Administration/dp/032194318X][The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2]]
+ [[http://www.amazon.com/NoSQL-Distilled-Emerging-Polyglot-Persistence/dp/0321826620][NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence]]
+ [[http://www.amazon.com/MongoDB-Applied-Design-Patterns-Copeland/dp/1449340040][MongoDB Applied Design Patterns]]
+ [[https://msdn.microsoft.com/en-us/library/dn568099.aspx][Cloud Design Patterns: Prescriptive Architecture Guidance for Cloud Applications]]

** [[https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf][Finding a needle in Haystack: Facebook’s photo storage]]
