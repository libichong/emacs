#+OPTIONS: toc:nil ^:nil author:nil date:nil html-postamble:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css" />
#+TITLE: 卷积神经网络和迁移学习

1. 不同层次结构有不同的形式（运算）和功能。
    #+BEGIN_EXAMPLE
    conv layer
    activation layer： Activation Function
    pooling layer
    fc layer
    #+END_EXAMPLE
2. 数据输入层
    #+BEGIN_EXAMPLE
    去均值
    把输入数据各个维度都中心化到0（训练集）
    减去训练集的均值（测试机）
    归一化scaling
    幅度初始化到同一范围内
    - PCA/白化
    #+END_EXAMPLE
3. 卷积计算层（feature extraction）
    #+BEGIN_EXAMPLE
    局部关联。每个神经元看做一个filter
    窗口：receptive field滑动，filter对局部数据计算
    涉及概念
    深度depth：当前这一conv layer的神经元的个数，即下一层的
    步长stride：决定时候有重叠
    填充值zero-padding：档窗口无法从最左侧滑动到最右侧0时，可以再在原始数据周围加n周0。
    参数共享机制（每个神经元以自己的方式观察这幅图）
    假设每个神经元连接所有数据窗的权重是固定的（否侧会极大的增大计算量级），不同神经元的权重是不一样的。
    固定每个神经元的连接权重，可以看做模板 每个神经元只关注一个特性
    需要估算的权重个数减少：AlexNet 1亿 =》 3.5w
    - 一组固定的权重和不同窗口内数据做內积：卷积
    #+END_EXAMPLE
4. 激励层（ReLU)
    #+BEGIN_EXAMPLE
    把卷基层输出结果做非线性映射：f()
    Sigmoid
    Tanh 双曲正切
    ReLU 修正线性单元
    leaky ReLU
    - Maxout
    #+END_EXAMPLE
5. 池化层Pooling layer（下采样）
    #+BEGIN_EXAMPLE
    夹在连续的卷基层中间
    - 压缩数据和参数量，减小过拟合
    maxPooling：取窗口中的maximum，imgprocess中用的较多的是maxpooling
    averagePooling
    randompooling
    #+END_EXAMPLE
6. 全连接FC层
    #+BEGIN_EXAMPLE
    两层之间所有的神经元都有权重连接
    通常全连接层在卷积神经网络尾部
    现在常用一个1*1的conv layer代替FC层
    INPUT
    [[CONV -> RELU] *Ｎ->POOLING?] *M
    fds
    - fdsa
    image -> convolution -> maxpooling -> convolution -> maxpooling -> flatten -> fully connected
    #+END_EXAMPLE
7. 对卷积层的理解
    #+BEGIN_EXAMPLE
    优点
    共享卷积和，对高维数据处理无压力
    无需手动选取特征，训练好权重，即得特征
    - 分类效果好
    缺点
    #+END_EXAMPLE
8. 典型网络
    #+BEGIN_EXAMPLE
    AlexNet：2012 ImageNet 第一名，Top5准确度超出第二10%， Error:16.4%
    ZFNet， 2013 ImageNet 第一名，Error ： 15.4% -> 14.8%
    VGG:
        VGG-16
        VGG-19
        - 。。。。。。
    GoogleNet: 2014 ImageNet 1st. Error: 6.67%
       - 将最后的FC层换成了几次呢过1*1的卷基层
    ResNet
        深层CNN训练困难在于梯度衰减（或者爆炸），即使有Batch Normalization，几十层的CNN也非常难训练
        如果离输入层太远，残差传回来信号太弱（失真）
        如果每一层学习一个独立的f(x)困难
        直接打通一条“皇城”到“蜀地”的快速通道，把input直接送到后面的层次。
        每次只要相对原数据，学习出来一个残差即可，而不是直接映射f(x)
    #+END_EXAMPLE
9. 图像相关任务
    #+BEGIN_EXAMPLE
    图像识别
    图像识别+定位
    思路1：视作回归问题
    思路2：图窗+识别与整合
    物体识别： 不知道有多少个主体
    边缘策略：EdgeBox
        selective search
        R-CNN -> fast-rcnn -> faster-rcnn RBG大神
        R-FCN：
    图像分割
    #+END_EXAMPLE
